"""
Agentes do sistema RAG ENEM.
Implementa os nÃ³s que compÃµem o grafo de processamento.
"""

import os
from typing import Dict, List, Any
from pathlib import Path
from langchain.schema import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
import chromadb
import json
import re
from dotenv import load_dotenv

# Carregar variÃ¡veis do .env
load_dotenv()


class ChromaDBRetriever:
    """Cliente para recuperaÃ§Ã£o de documentos do ChromaDB."""
    
    def __init__(
        self,
        collection_name: str = "enem_documents",
        embedding_model: str = "BAAI/bge-m3",
        use_cloud: bool = True,
        chroma_api_key: str = None,
        chroma_tenant: str = None,
        chroma_database: str = "enem_rag"
    ):
        self.collection_name = collection_name
        self.embedding_model = embedding_model
        self.use_cloud = use_cloud
        
        # ConfiguraÃ§Ãµes do Chroma Cloud do .env
        self.chroma_api_key = chroma_api_key or os.getenv('CHROMA_API_KEY')
        self.chroma_tenant = chroma_tenant or os.getenv('CHROMA_TENANT')
        self.chroma_database = chroma_database or os.getenv('CHROMA_DATABASE', 'enem_rag')
        
        # Configurar modelo de embeddings
        self.embeddings_model = HuggingFaceEmbeddings(
            model_name=embedding_model,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # Configurar ChromaDB
        self._setup_chromadb()
    
    def _setup_chromadb(self):
        """Configura conexÃ£o com ChromaDB."""
        try:
            if self.use_cloud and self.chroma_api_key and self.chroma_tenant:
                self.chroma_client = chromadb.CloudClient(
                    api_key=self.chroma_api_key,
                    tenant=self.chroma_tenant,
                    database=self.chroma_database
                )
            else:
                self.chroma_client = chromadb.PersistentClient(path="vector_store")
            
            self.collection = self.chroma_client.get_collection(name=self.collection_name)
            
        except Exception as e:
            raise RuntimeError(f"Erro ao conectar ChromaDB: {str(e)}")
    
    def retrieve(self, query: str, k: int = 5, filters: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """
        Recupera documentos similares Ã  query.
        
        Args:
            query: Pergunta do usuÃ¡rio
            k: NÃºmero de documentos a retornar
            filters: Filtros de metadados
            
        Returns:
            Lista de documentos recuperados
        """
        try:
            # Gerar embedding da query manualmente
            query_embedding = self.embeddings_model.embed_documents([query])[0]
            
            # Preparar parÃ¢metros da query
            query_params = {
                "query_embeddings": [query_embedding],
                "n_results": k,
                "include": ["documents", "metadatas", "distances"]
            }
            
            if filters:
                query_params["where"] = filters
            
            # Realizar busca
            results = self.collection.query(**query_params)
            
            # Formatar resultados
            documents = []
            if results['documents'] and results['documents'][0]:
                for i in range(len(results['documents'][0])):
                    doc = {
                        'content': results['documents'][0][i],
                        'metadata': results['metadatas'][0][i],
                        'similarity_score': 1 - results['distances'][0][i],
                        'source': results['metadatas'][0][i].get('source', 'N/A'),
                        'page': results['metadatas'][0][i].get('page', 'N/A')
                    }
                    documents.append(doc)
            
            return documents
            
        except Exception as e:
            print(f"Erro na recuperaÃ§Ã£o: {str(e)}")
            return []


# InstÃ¢ncia global do retriever (serÃ¡ inicializada quando necessÃ¡rio)
_retriever = None

def get_retriever() -> ChromaDBRetriever:
    """ObtÃ©m instÃ¢ncia global do retriever."""
    global _retriever
    if _retriever is None:
        _retriever = ChromaDBRetriever(
            use_cloud=os.getenv('USE_CHROMA_CLOUD', 'true').lower() == 'true',
            chroma_api_key=os.getenv('CHROMA_API_KEY'),
            chroma_tenant=os.getenv('CHROMA_TENANT')
        )
    return _retriever


def retrieve_documents(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    NÃ³ recuperador: busca documentos relevantes no ChromaDB.
    
    Args:
        state: Estado atual do grafo
        
    Returns:
        Estado atualizado com documentos recuperados
    """
    question = state.get("question", "")
    metadata = state.get("metadata", {})
    
    print(f"ğŸ” [RETRIEVE] Buscando documentos para: '{question[:50]}...'")
    
    try:
        # Obter retriever
        retriever = get_retriever()
        
        # Extrair filtros dos metadados se fornecidos
        filters = metadata.get("filters", {})
        k = metadata.get("max_documents", int(os.getenv('MAX_DOCUMENTS', '5')))
        
        # Recuperar documentos
        documents = retriever.retrieve(query=question, k=k, filters=filters)
        
        print(f"ğŸ“š [RETRIEVE] Encontrados {len(documents)} documentos relevantes")
        
        # Atualizar estado
        state["documents"] = documents
        
        return state
        
    except Exception as e:
        print(f"âŒ [RETRIEVE] Erro: {str(e)}")
        state["documents"] = []
        return state


def generate_answer(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    NÃ³ gerador: cria resposta baseada nos documentos recuperados.
    
    Args:
        state: Estado atual do grafo
        
    Returns:
        Estado atualizado com resposta gerada
    """
    question = state.get("question", "")
    documents = state.get("documents", [])
    
    print(f"ğŸ¤– [GENERATE] Gerando resposta baseada em {len(documents)} documentos")
    
    # Template de prompt para geraÃ§Ã£o de resposta
    prompt_template = PromptTemplate(
        input_variables=["question", "context"],
        template="""
VocÃª Ã© um assistente especializado em informaÃ§Ãµes sobre o ENEM (Exame Nacional do Ensino MÃ©dio).

Sua tarefa Ã© responder Ã  pergunta do usuÃ¡rio usando ESTRITAMENTE as informaÃ§Ãµes fornecidas nos documentos abaixo.

REGRAS IMPORTANTES:
1. Use APENAS as informaÃ§Ãµes dos documentos fornecidos
2. Se a informaÃ§Ã£o nÃ£o estiver nos documentos, responda: "NÃ£o encontrei informaÃ§Ãµes sobre isso nas fontes fornecidas"
3. Para cada afirmaÃ§Ã£o, cite a fonte no formato [Fonte: nome_do_arquivo, PÃ¡gina: X]
4. Seja preciso e objetivo
5. NÃ£o invente ou assuma informaÃ§Ãµes que nÃ£o estejam explicitamente nos documentos
6. Seja direto e utilize apenas os dados fornecidos pelos documentos

DOCUMENTOS FORNECIDOS:
{context}

PERGUNTA: {question}

RESPOSTA:
"""
    )
    
    try:
        # Preparar contexto dos documentos
        context_parts = []
        for i, doc in enumerate(documents[:5]):  # Limitar a 5 documentos
            context_parts.append(
                f"Documento {i+1}:\n"
                f"Fonte: {doc['source']}, PÃ¡gina: {doc['page']}\n"
                f"ConteÃºdo: {doc['content'][:1000]}...\n"
                f"---"
            )
        
        context = "\n".join(context_parts) if context_parts else "Nenhum documento relevante encontrado."
        
        # Configurar LLM com Google Gemini usando .env
        llm = ChatGoogleGenerativeAI(
            model=os.getenv('GEMINI_MODEL', 'gemini-pro'),
            temperature=float(os.getenv('LLM_TEMPERATURE', '0.1')),
            google_api_key=os.getenv('GOOGLE_API_KEY'),
            convert_system_message_to_human=True
        )
        
        # Gerar resposta
        prompt = prompt_template.format(question=question, context=context)
        response = llm.invoke(prompt)
        
        answer = response.content if hasattr(response, 'content') else str(response)
        
        print(f"âœ… [GENERATE] Resposta gerada com {len(answer)} caracteres")
        
        # Atualizar estado
        state["answer"] = answer
        
        return state
        
    except Exception as e:
        print(f"âŒ [GENERATE] Erro: {str(e)}")
        state["answer"] = "Desculpe, ocorreu um erro ao gerar a resposta."
        return state


def check_groundedness(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    NÃ³ verificador: verifica se a resposta estÃ¡ bem fundamentada nos documentos.
    
    Args:
        state: Estado atual do grafo
        
    Returns:
        Estado atualizado com verificaÃ§Ã£o de fundamentaÃ§Ã£o
    """
    answer = state.get("answer", "")
    documents = state.get("documents", [])
    
    print(f"ğŸ” [CHECK] Verificando fundamentaÃ§Ã£o da resposta")
    
    # Template de prompt para verificaÃ§Ã£o mais permissivo
    prompt_template = PromptTemplate(
        input_variables=["answer", "context"],
        template="""
VocÃª deve verificar se a resposta fornecida Ã© razoavelmente suportada pelos documentos de contexto.

DOCUMENTOS DE CONTEXTO:
{context}

RESPOSTA A VERIFICAR:
{answer}

CRITÃ‰RIOS PARA APROVAÃ‡ÃƒO (responda 'sim' se QUALQUER um dos critÃ©rios for atendido):

1. INFORMAÃ‡Ã•ES DIRETAS: A resposta contÃ©m informaÃ§Ãµes que aparecem diretamente nos documentos
2. INFERÃŠNCIAS VÃLIDAS: A resposta faz inferÃªncias razoÃ¡veis baseadas no contexto dos documentos
3. RESPOSTAS PARCIAIS: A resposta fornece informaÃ§Ãµes parciais que estÃ£o nos documentos, mesmo que incompletas
4. CITAÃ‡Ã•ES CORRETAS: A resposta cita fontes corretas dos documentos fornecidos
5. AUSÃŠNCIA DECLARADA: A resposta honestamente declara que nÃ£o encontrou informaÃ§Ãµes especÃ­ficas
6. INFORMAÃ‡Ã•ES RELACIONADAS: A resposta fornece informaÃ§Ãµes relacionadas ao tÃ³pico que estÃ£o presentes nos documentos

APENAS responda 'nÃ£o' se:
- A resposta contÃ©m informaÃ§Ãµes completamente inventadas que NÃƒO estÃ£o nos documentos
- A resposta contradiz diretamente as informaÃ§Ãµes dos documentos
- A resposta nÃ£o tem NENHUMA relaÃ§Ã£o com o conteÃºdo fornecido

Seja PERMISSIVO e considere que o assistente estÃ¡ tentando ser Ãºtil com base nas informaÃ§Ãµes disponÃ­veis.

Responda APENAS com 'sim' ou 'nÃ£o':
"""
    )
    
    try:
        # Se nÃ£o hÃ¡ resposta ou documentos, considerar nÃ£o fundamentada
        if not answer.strip() or not documents:
            state["is_grounded"] = False
            print("âŒ [CHECK] NÃ£o fundamentada: sem resposta ou documentos")
            return state
        
        # Se a resposta indica que nÃ£o encontrou informaÃ§Ãµes, considerar fundamentada
        if any(phrase in answer.lower() for phrase in [
            "nÃ£o encontrei informaÃ§Ãµes",
            "nÃ£o hÃ¡ informaÃ§Ãµes",
            "nÃ£o localizei",
            "nÃ£o consta",
            "nÃ£o foi possÃ­vel encontrar"
        ]):
            state["is_grounded"] = True
            print("âœ… [CHECK] Fundamentada: resposta indica ausÃªncia de informaÃ§Ãµes")
            return state
        
        # Se a resposta contÃ©m citaÃ§Ãµes de fontes vÃ¡lidas, considerar fundamentada
        if any(doc['source'] in answer for doc in documents[:5]):
            state["is_grounded"] = True
            print("âœ… [CHECK] Fundamentada: resposta contÃ©m citaÃ§Ãµes de fontes vÃ¡lidas")
            return state
        
        # Preparar contexto para verificaÃ§Ã£o (usar mais contexto para ser mais permissivo)
        context_parts = []
        for doc in documents[:5]:  # Usar os 5 mais relevantes para dar mais contexto
            context_parts.append(f"- {doc['content'][:800]}...")  # Aumentar o tamanho do contexto
        
        context = "\n".join(context_parts)
        
        # Configurar LLM para verificaÃ§Ã£o com Google Gemini usando .env
        llm = ChatGoogleGenerativeAI(
            model=os.getenv('GEMINI_MODEL', 'gemini-pro'),
            temperature=0.0,
            google_api_key=os.getenv('GOOGLE_API_KEY'),
            convert_system_message_to_human=True
        )
        
        # Verificar fundamentaÃ§Ã£o
        prompt = prompt_template.format(answer=answer, context=context)
        response = llm.invoke(prompt)
        
        verification = response.content if hasattr(response, 'content') else str(response)
        verification = verification.strip().lower()
        
        # Ser mais permissivo na interpretaÃ§Ã£o
        is_grounded = any(word in verification for word in ['sim', 'yes', 'vÃ¡lid', 'correto', 'fundamentad'])
        
        # Se ainda nÃ£o foi aprovado, fazer uma verificaÃ§Ã£o adicional mais flexÃ­vel
        if not is_grounded:
            # VerificaÃ§Ã£o adicional: se a resposta tem alguma relaÃ§Ã£o com o contexto
            answer_words = set(answer.lower().split())
            context_words = set(context.lower().split())
            
            # Se hÃ¡ sobreposiÃ§Ã£o significativa de palavras-chave
            overlap = len(answer_words.intersection(context_words))
            if overlap > 5:  # Threshold flexÃ­vel
                is_grounded = True
                print("âœ… [CHECK] Fundamentada: sobreposiÃ§Ã£o significativa de conteÃºdo")
        
        state["is_grounded"] = is_grounded
        
        status = "âœ… Fundamentada" if is_grounded else "âŒ NÃ£o fundamentada"
        print(f"[CHECK] {status} (verificaÃ§Ã£o: {verification[:100]}...)")
        
        return state
        
    except Exception as e:
        print(f"âŒ [CHECK] Erro na verificaÃ§Ã£o: {str(e)}")
        # Em caso de erro, considerar fundamentada para nÃ£o bloquear
        state["is_grounded"] = True
        return state


def apply_safety_layer(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    NÃ³ de seguranÃ§a: adiciona disclaimer Ã  resposta fundamentada.
    
    Args:
        state: Estado atual do grafo
        
    Returns:
        Estado atualizado com resposta final e disclaimer
    """
    answer = state.get("answer", "")
    
    print(f"ğŸ›¡ï¸ [SAFETY] Aplicando camada de seguranÃ§a")
    
    # Disclaimer padrÃ£o
    disclaimer = (
        "\n\n---\n"
        "âš ï¸ **AVISO**: Este Ã© um assistente experimental e uma prova de conceito. "
        "As informaÃ§Ãµes sÃ£o extraÃ­das de documentos oficiais, mas podem conter imprecisÃµes. "
        "Sempre consulte as fontes originais do INEP/MEC para informaÃ§Ãµes definitivas."
    )
    
    # Construir resposta final
    final_response = answer + disclaimer
    
    # Atualizar estado
    state["final_response"] = final_response
    
    print(f"âœ… [SAFETY] Disclaimer adicionado Ã  resposta final")
    
    return state


def answer_question(question: str, filters: Dict[str, Any] = None) -> str:
    """
    FunÃ§Ã£o simples para responder perguntas (compatibilidade com app_streamlit.py).
    
    Args:
        question: Pergunta do usuÃ¡rio
        filters: Filtros opcionais
        
    Returns:
        Resposta processada
    """
    try:
        # Executar pipeline manualmente
        state = {"question": question, "documents": [], "answer": "", "is_grounded": False, "final_response": "", "metadata": {"filters": filters or {}}}
        
        # Executar nÃ³s em sequÃªncia
        state = retrieve_documents(state)
        state = generate_answer(state)
        state = check_groundedness(state)
        
        if state.get("is_grounded", False):
            state = apply_safety_layer(state)
            return state.get("final_response", "Erro ao gerar resposta")
        else:
            return "NÃ£o encontrei informaÃ§Ãµes suficientes nas fontes fornecidas para responder sua pergunta."
            
    except Exception as e:
        return f"Erro ao processar pergunta: {str(e)}"